---
title: Visualising Model Response with easyalluvial
author: Bjönr Koneswarakantha
date: '2019-03-29'
slug: visualising-model-response-with-easyalluvial
categories:
  - R
  - visualisation
tags:
  - R
  - visualisation
  - partial dependency plots
  - easyalluvial
  - caret
  - model response
keywords:
  - tech
summary: Use easyalluvial for visualising model response in up to 4 dimensions.
thumbnailImagePosition : left
thumbnailImage: easyalluvial_logo.png
editor_options: 
  chunk_output_type: console
output:
  output: blogdown::html_page
---



<p>In this tutorial I want to show how you can use alluvial plots to visualise model response in up to 4 dimensions. <code>easyalluvial</code> generates artificial data space using fixed values for unplotted variables or uses the partial dependence plotting method. It is model agnostic but offers some convenient wrappers for <code>caret</code> models. <!--more--></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div id="taking-a-peek" class="section level2">
<h2>Taking a peek</h2>
<p>When building machine learning model we are usually faced with a trade-off between accurracy and interpretability. However even if we tend to lean towards accuracy and pick a modelling method that results in nearly uninterpretable models we can still make use of a bunch of model agnostic techniques that have been summarized in this excellent ebook <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning by Christoph Molnar</a>.</p>
<p>Whithout getting to theoretical I personally always feel the urge to simply take a peek simulate some data and see how the model reacts a method described in <a href="http://vita.had.co.nz/papers/model-vis.pdf">Wickham H, Cook D, Hofmann H (2015) Visualizing statistical models: Removing the blindfold. Statistical Analysis and Data Mining 8(4) &lt;doi:10.1002/sam.11271&gt;</a>. In order to simulate data we can generate a a vector with a sequence of values over the entire range of a predictor variable of interest while setting all the others to their median or mode and use this artificial data space to obtain model predictions which we can plot against our variable of interest. An R package that will do this for you is (<code>plotmo</code>)[<a href="https://cran.r-project.org/web/packages/plotmo/index.html" class="uri">https://cran.r-project.org/web/packages/plotmo/index.html</a>]. Instead of ranging over 1 predictor variable we can create a data grid using 2 predictor variables and plot the response as a third dimension. However this is as far as you can go in a conventional plots. Alluvial plots can line up much more than 3 dimensions on a plane next to each other only limited by the number of flows as it will get too cluttered when there are too many of them.</p>
</div>
<div id="which-variables-to-plot" class="section level2">
<h2>Which variables to plot</h2>
<p>When using conventional model response plotting beeing limited two 2 variables we can simply resolve this by generating many plots and look at them one by one. Alluvial plots require a bit more attention and cannot easily be screened and compared since visually there is so much going on. Therefore I do not recommend to brute force it by simply creating a lot of random combinations of predictor variables and multiple alluvial plots but instead to focus on those that have the highest calculated feature importance. Feature importance values are natively provided by most modelling packages. So the question is how many can we plot and it turns out 4 features will usually result in well interpretable plot.</p>
</div>
<div id="generating-the-data-space" class="section level2">
<h2>Generating the data space</h2>
<pre class="r"><code>suppressPackageStartupMessages( require(tidyverse) )
suppressPackageStartupMessages( require(easyalluvial) )
suppressPackageStartupMessages( require(mlbench) )
suppressPackageStartupMessages( require(randomForest) )</code></pre>
<p>We start by creating a model</p>
<pre class="r"><code>data(&#39;BostonHousing&#39;)
df = as_tibble( BostonHousing )
m = randomForest( lstat ~ ., df )</code></pre>
<p>and looking at the importance features</p>
<pre class="r"><code>imp = m$importance %&gt;%
  tidy_imp(df) # 

knitr::kable(imp)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">vars</th>
<th align="right">imp</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">medv</td>
<td align="right">8331.8325</td>
</tr>
<tr class="even">
<td align="left">rm</td>
<td align="right">3554.7917</td>
</tr>
<tr class="odd">
<td align="left">age</td>
<td align="right">2697.7033</td>
</tr>
<tr class="even">
<td align="left">indus</td>
<td align="right">2649.3554</td>
</tr>
<tr class="odd">
<td align="left">crim</td>
<td align="right">2306.9112</td>
</tr>
<tr class="even">
<td align="left">dis</td>
<td align="right">1847.7221</td>
</tr>
<tr class="odd">
<td align="left">nox</td>
<td align="right">1547.4916</td>
</tr>
<tr class="even">
<td align="left">b</td>
<td align="right">742.3811</td>
</tr>
<tr class="odd">
<td align="left">tax</td>
<td align="right">632.3429</td>
</tr>
<tr class="even">
<td align="left">ptratio</td>
<td align="right">308.5192</td>
</tr>
<tr class="odd">
<td align="left">rad</td>
<td align="right">201.1113</td>
</tr>
<tr class="even">
<td align="left">zn</td>
<td align="right">135.7454</td>
</tr>
<tr class="odd">
<td align="left">chas</td>
<td align="right">106.6607</td>
</tr>
</tbody>
</table>
<p>When generating the data space we cannot screent and infinite amount of values per variable. We want to create all possible combinations between the values of the 4 variables we want to plot and an alluvial plot we cannot distinguish more than 1000 flows I recommend to go with 5 values which will result in 5 x 5 X 5 X 5 –&gt; 625 combinations. That also leaves some wiggeling room if one of the top 4 variables is a factor with more than 5 levels. <code>get_data_space()</code> will split the range of a variable into 3 and picks the median of each split and add the variable minimum and the maximum to the set.</p>
<pre class="r"><code>dspace = get_data_space(df, imp
                        , degree = 4 # specifies the number of variables
                        , bins = 5 # the number of values per variable
                        )

knitr::kable( head(dspace, 10) )</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">medv</th>
<th align="right">rm</th>
<th align="right">age</th>
<th align="right">indus</th>
<th align="right">crim</th>
<th align="right">dis</th>
<th align="right">nox</th>
<th align="right">b</th>
<th align="right">tax</th>
<th align="right">ptratio</th>
<th align="right">rad</th>
<th align="right">zn</th>
<th align="left">chas</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">2.9</td>
<td align="right">0.46</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">2.9</td>
<td align="right">4.27</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">2.9</td>
<td align="right">9.90</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">2.9</td>
<td align="right">18.10</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">2.9</td>
<td align="right">27.74</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">32.2</td>
<td align="right">0.46</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">32.2</td>
<td align="right">4.27</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">32.2</td>
<td align="right">9.90</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">32.2</td>
<td align="right">18.10</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3.561</td>
<td align="right">32.2</td>
<td align="right">27.74</td>
<td align="right">0.25651</td>
<td align="right">3.20745</td>
<td align="right">0.538</td>
<td align="right">391.44</td>
<td align="right">330</td>
<td align="right">19.05</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>Total rows in dspace: 625</p>
<pre class="r"><code>dspace %&gt;%
  summarise_all( ~ length( unique(.) ) ) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">medv</th>
<th align="right">rm</th>
<th align="right">age</th>
<th align="right">indus</th>
<th align="right">crim</th>
<th align="right">dis</th>
<th align="right">nox</th>
<th align="right">b</th>
<th align="right">tax</th>
<th align="right">ptratio</th>
<th align="right">rad</th>
<th align="right">zn</th>
<th align="right">chas</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
</div>
<div id="generating-model-response" class="section level2">
<h2>Generating model response</h2>
<pre class="r"><code>pred = predict(m, newdata = dspace)</code></pre>
</div>
<div id="plotting" class="section level2">
<h2>Plotting</h2>
<p>The predictions will be binned as well into 5 bins. Binning options can be passed as a list via the <code>params_bin_numeric_pred</code> parameter.</p>
<pre class="r"><code>p = alluvial_model_response(pred, dspace, imp
                            , degree = 4, bins = 5
                            , stratum_label_size = 2.8 )
p</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/alluv1-1.png" width="960" /></p>
<p>We can seet the binned predictions and by which variable combination they have been created by tracing the coloured flows. The stratum labels of the prediction variables indicate the value of the variable and which fraction of the flows of that colour (prediction variable bin range) pass through that stratum.</p>
</div>
<div id="marginal-histograms" class="section level2">
<h2>Marginal histograms</h2>
<p>As well as for other <code>easyalluvial</code> plots we can add marginal histograms and as a bonus also the feature importance.</p>
<pre class="r"><code>p_grid = add_marginal_histograms(p, data_input = df
                                 , plot = F # plot only after adding feature importance
                                 , scale = 50 # to increase distance between ridge plots, Default: 400
                                 ) %&gt;%
  add_imp_plot( p = p, data_input = df)</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/unnamed-chunk-7-1.png" width="1152" /></p>
<p>We can see the original distribution of the variables and the lines indicate the position of the values as picked by <code>get_data_space()</code>. When comparing the distribution of the predictions against the original distribution of <code>lstat</code> we see that the range of the predictions in response to the artificial dataspace do not cover all of the range of <code>lstat</code>. Which most likely means that all possible combinations of the 4 plotted variables in combination with moderate values for all other predictors will not give any extreme values. But first we will need to check whether the model is capable of making predictions in the lower and upper ranges of <code>lsat</code>. We can use <code>plot_hist()</code> to only plot the distributions and add the prediction for the training data set using the <code>pred_train</code> parameter.</p>
<pre class="r"><code>pred_train = predict(m)

plot_hist(&#39;pred&#39;, p, df
          , pred_train = pred_train # pred_train can also be passed to add_marginal_histograms()
          , scale = 50)</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/unnamed-chunk-8-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>We see that the training prediction also do not completely cover all of <code>lstat</code> range but more of it than the predictions from the aritficial data space.</p>
<p>If we wanted to emphasize this we can bin the data space predictions on the basis of the training predictions. In this case it makes sense to increase the number of bins for the predictions in order not to loose resolution on the plot.</p>
<pre class="r"><code>p = alluvial_model_response(pred, dspace, imp, degree = 4, bins = 7
                            , bin_labels = c(&quot;LLL&quot;,&quot;LL&quot;, &quot;ML&quot;, &quot;M&quot;, &quot;MH&quot;, &quot;HH&quot;, &quot;HHH&quot;)
                            , pred_train = pred_train
                            , stratum_label_size = 2.8 )

plot_hist(&#39;pred&#39;, p, df, scale = 50)</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/p_hist-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>p_grid = add_marginal_histograms(p, data_input = df
                                 , plot = F # plot only after adding feature importance
                                 , scale = 50 # to increase distance between ridge plots, Default: 400
                                 ) %&gt;%
  add_imp_plot( p = p, data_input = df)</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/unnamed-chunk-9-1.png" width="1152" /></p>
</div>
</div>
<div id="what-feature-combinations-are-needed-to-obtain-predictions-in-the-lower-and-higher-ranges-of-lstat" class="section level1">
<h1>What feature combinations are needed to obtain predictions in the lower and higher ranges of <code>lstat</code>?</h1>
<p>We can add the training predictions to the training data and assign the variables not covered by the model response plot to bins. We can then create an alluvial plot over the entire training dataframe. Since we are only interested in those observations from the training data that cause predictions not covered by the model response plot we remove all other flows from the plot by setting their color to <code>NA</code>.</p>
<pre class="r"><code>breaks = c( min(pred_train) - 1,min(pred),max(pred),max(pred_train) + 1 )

df_inv = df %&gt;%
  select(-lstat) %&gt;%
  mutate( pred_train = pred_train
          , pred_train = cut(pred_train, breaks) ) %&gt;%
  select( pred_train, one_of(imp$vars) )

p = alluvial_wide(df_inv, bin_labels = &#39;min_max&#39;
                  , stratum_label_size = 3
                  , col_vector_flow = c(&#39;blue&#39;, NA, &#39;orange&#39;)
                  , colorful_fill_variable_stratum = F)

p_grid = add_marginal_histograms(p, df_inv)</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/unnamed-chunk-10-1.png" width="1152" /></p>
</div>
<div id="caret-wrapper" class="section level1">
<h1><code>caret</code> wrapper</h1>
<p><code>caret</code> provides a uniformous interface for training and calling a lot of machine learning models as well as for calculating feature importance. <code>easyalluvial</code> provides a function that wraps the above described workflow into one single call. Note that feature importance values are slightly different. <code>randomForest</code> returns a combined importance for all levels of a factor variable while <code>caret</code> splits them up. <code>tidy_imp()</code> aggregates them and uses the maximum value of all values. This behaviour can be adjusted by passing another aggregating function to the <code>.f</code> parameter.</p>
<pre class="r"><code>train = caret::train( lstat ~ .
                     , df
                     , method = &#39;rf&#39;
                     , trControl = caret::trainControl( method = &#39;none&#39; )
                     , importance = TRUE )

alluvial_model_response_caret(train, degree = 4, bins = 5
                              , stratum_label_size = 2.8)</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/unnamed-chunk-11-1.png" width="960" /></p>
</div>
<div id="advantages" class="section level1">
<h1>Advantages</h1>
<p>Model response alluvial plots can help us to get an immediate intuitive understanding how predictions of a certain range can be generated by the model. They can be understood by none-statistical stakeholders and invite the viewer to start exploring and question the decision making process of the model while also convey an appreciation for the model complexity as flows branch out to the variables of lower feature importance.</p>
</div>
<div id="partial-dependence-plotting-method" class="section level1">
<h1>Partial Dependence Plotting Method</h1>
<ol style="list-style-type: decimal">
<li><p>construct the data space as described above, but instead of setting all none-plotted variables to median/mode we set them to the values found in the first row of the training data.</p></li>
<li><p>Repeat 1. for each row</p></li>
<li><p>Use all data spaces to get model predictions</p></li>
<li><p>Average model predictions</p></li>
</ol>
<p><code>get_pdp_predictions()</code> will do this for us, however we need to provide it with the prediction function appropriate for the type of the model. Usually those functions are not exported by the package but can be found using <code>:::</code>.</p>
<pre class="r"><code>pred = get_pdp_predictions(df, imp, m, degree = 4, bins = 5)


alluvial_model_response(pred, dspace, imp, degree = 4
                        , method = &#39;pdp&#39; # changes title and caption
                        , stratum_label_size = 2.8 )</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/pdp-1.png" width="960" /></p>
<p>thanks to the uniformous modelling interface of <code>caret</code> <code>alluvial_model_response_caret()</code> can call <code>get_pdp_predictions()</code> directly.</p>
<pre class="r"><code>alluvial_model_response_caret(train, degree = 4, bins = 5
                              , method = &#39;pdp&#39;
                              , stratum_label_size = 2.8 )</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/pdp_caret-1.png" width="960" /></p>
<div id="a-note-on-feature-importance" class="section level2">
<h2>A note on feature importance</h2>
<p>Calculating feature importance in a dataset with strongly correlating variables will lead to inacurrate results. It usually means that two strongly correlating variables share the importance that would be accredited to them if only one of them was present in the data set. Further some feature importance calculation methods require the model to be trained with scaled, centered and potentially normalized (transformed to be more normally distributed) numerical variables in order to deliver meaningful results. We can use the <code>recipes</code> package to perform these tasks. Using the below described workflow we can create the artifical dataspace with untransformed numerical values which will give us more meaningful output.</p>
<pre class="r"><code>suppressPackageStartupMessages( require(recipes) )


# filter correlating variables recipe- ------------------------------
# caret tends to complain if recipe filters any variables, therefore
# we use two recipes

rec_filt_corrs = recipe(df, lstat ~ . ) %&gt;%
  step_corr( all_numeric()
             , - all_outcomes()
             , threshold = 0.7 ) %&gt;%
  prep()


df_filt_corrs = bake(rec_filt_corrs, df)

# transformation recipe --------------------------------------------
rec_trans = recipe( df_filt_corrs, lstat ~ .) %&gt;%
  step_center( all_numeric(), - all_outcomes() ) %&gt;%
  step_scale( all_numeric(), - all_outcomes() ) %&gt;%
  step_YeoJohnson( all_numeric(), - all_outcomes() ) %&gt;%
  prep()

# train and plot ---------------------------------------------------
set.seed(1)
train = caret::train( rec_trans
                     , df_filt_corrs
                     , method = &#39;earth&#39;
                     , trControl = caret::trainControl( method = &#39;cv&#39;
                                                        , search = &#39;random&#39;)
                     , tuneLength = 100)

pred_train = predict(train, df_filt_corrs)

p = alluvial_model_response_caret(train, degree = 4, bins = 5
                                  , pred_train = pred_train
                                  , stratum_label_size = 2.8 )

p_grid = add_marginal_histograms(p, df, plot = F
                                 , pred_var = &#39;lstat&#39;
                                 , scale = 50) %&gt;%
  add_imp_plot(p, df)</code></pre>
<p><img src="/page/2019-03-29-visualising-model-response-with-easyalluvial_files/figure-html/unnamed-chunk-12-1.png" width="1152" /></p>
</div>
<div id="limitations" class="section level2">
<h2>Limitations</h2>
<ul>
<li>There is a loss of information when binning the numerical variables</li>
<li>The combinations generated when making the grid might be outside the feature distribution space (generate combinations that are impossible)</li>
<li>We only look at the combination of 4 features and disregard the others</li>
</ul>
<p>To alleviate this you can reduce the complexity of the model by reducing features (take out correlating variables) or use additional model exploration methods such as classical PDPs, ALE plots, Shapely values, etc, …</p>
</div>
</div>
