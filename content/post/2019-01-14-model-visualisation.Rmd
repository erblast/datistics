---
title: Model Visualisation
author: Bj√∂rn Koneswarakantha
date: '2019-01-14'
slug: model-visualisation
categories:
  - R
tags:
  - modelling
  - visualisation
  - data exploration
---


# Lifting the veil of black box models
Modern ensemble machine learning techniques produce uninterpretable models and it can be difficult to judge which patterns in the data the model recognizes and uses to make predictions. This reduces overall confidence in the model for business stakeholders and we can never fully exclude that the model will create unsensible predictions especially when confronted with unknown data patterns. It is thus important that one understands the logic of the model and are able to judge and explain the prediction that the model makes.
 
# "data in the model space" vs. "model in the data space"
In order to check the quality of a model we usually only visualize the model predictions made in response to the observations in the training data, which can be regarded as *visualising the data in the model space*. Herein lies the risk, as the training data is unlikely to be a complete representation of all future data that the model will encounter. In order to turn the perspective we can *visualise the model in the data space*, meaning that we can explore the model along the logical ranges of the dimensions found in our data. The logical ranges being all values that a certain dimension/variable could possibly take on. [see Wickham et al. "Visualizing statistical models: Removing the blindfold, 2015"](http://vita.had.co.nz/papers/model-vis.html)
To sufficiently explore the model respones to changes in the hyperdimensional data space interactive visual tools are most powerfull, however several more generalistic 2D visualisation can cover a lot of ground. As realised for example in the [plotmo R package](http://www.milbo.org/doc/plotmo-notes.pdf). For `sherlock` we currently explore the model response (1<sup>st</sup> dimension) related to a predetermined dimension x (2<sup>nd</sup> dimension) against all other possible dimenstions (multiple 3<sup>rd</sup> dimensions).

# Explaining and Interpreting predictions
Visualising the model in the data space will make us able to intuitively explain a large number of the model predictions. It will however be impossible to account for all possible predictions therefore we can add a second technique to our repertoire. We assume that observations that are relatively close in the hyperdimensional space also produce predictions that are very close. Therefore we can take an observation and make gradual changes to the local data space of that observation and measure the changes in the response of the model. From this we can learn the weight of each of the dimensions in creating the particular response to the observation that we are investigating. This principle is applied by the [LIME algorithm](https://www.data-imaginist.com/2017/announcing-lime/).
# Reducing model complexity
Finally understanding the logic of the model, can help reducing the model complexity by either dropping variables that do not envoke a relevant response from the model, by introducing a sensible binning of existing variables, or by switching from an ensembl model with hundreds of stacked models to a combination of two or three simpler models that are intrinsically interpretable.

# Visualising the model in the data space in `R`
- model response plots 1D and surface plots 2D with `plotmo`
- alluvial plots 5D with `easyalluvial`
- randomized response plots all dimensions
