{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced scikitlearn\n",
    "\n",
    "We have seen in the last document, some advantages of `scikitlearn`. Most notably the seamless integration of parallel processing. I was struggeling a bit with the fact that `scikitlearn` only accepts `numpy` arrays as input and I was missing the `recipes` package which makes initial data transformation in `R` so much easier. Then I stumbled upon `sklearn-pandas` which seamlessly integrates `pandas` with `sklearn` and supports a pipe based workflow, which is a `sklearn` feaure I have not started to explore yet. To my understanding the pipe workflow in `sklearn` is an object-oriented pythonic version of the modelling dataframe concept introduced by the `tidyverse` in `R`. Finally there are packages like `TPOT` and `auto-sklearn` which automate the whole thing.\n",
    "\n",
    "In general there are a number of projects that use the synthax and structure of scikit learn, a collection of them can be found at\n",
    "\n",
    "-[http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html](https://github.com/scikit-learn-contrib/scikit-learn-contrib/blob/master/README.md)\n",
    "\n",
    "-[http://scikit-learn.org/stable/related_projects.html](http://scikit-learn.org/stable/related_projects.html)\n",
    "\n",
    "\n",
    "# To sparse or not to sparse\n",
    "\n",
    "In the `python` data world data is considered to be *sparse* or *dense*. Which adresses the number of zeros in a matrix [wiki](https://en.wikipedia.org/wiki/Sparse_matrix). *sparse* means that you have a lot of them while *dense* means the opposite. There is no particular threshold but we should be aware that some data transformatios like dummy encoding make our data more *sparse*. A *sparse* matrix can be stored in a more memory efficient format such similar as a compressed image file and some algorithms can computationally leaverage this format to reduce computing time. [lasso](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_dense_vs_sparse_data.html) and [boosting gradient style algorithms](https://medium.com/sbc-group-blog/to-sparse-or-not-to-sparse-691483f87a53) seem to be able to profit from the sparse data format while others [neural nets, knn](https://medium.com/sbc-group-blog/to-sparse-or-not-to-sparse-691483f87a53) do not, and some like [randomForest](https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required) require the regular dense format and will otherwise raise an error. We can use `SciPy` to transform matrices to a dense format. We can measure the sparcity ratio as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_ratio(X):\n",
    "    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## `sklearn-pandas`\n",
    "\n",
    "- [github](https://github.com/scikit-learn-contrib/sklearn-pandas)\n",
    "\n",
    "Core of this package is the `DataFrameMapper` class which maps scikit learn Transformer classes to specific columns of a dataframe and outputs either a numpy array or dataframe.\n",
    "\n",
    "Additionally it provides a `CategoricalImputer` which accepts categorical data, which I had to write myself before in the last document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>class_First</th>\n",
       "      <th>class_Second</th>\n",
       "      <th>class_Third</th>\n",
       "      <th>who_child</th>\n",
       "      <th>who_man</th>\n",
       "      <th>who_woman</th>\n",
       "      <th>...</th>\n",
       "      <th>deck_G</th>\n",
       "      <th>embark_town_Cherbourg</th>\n",
       "      <th>embark_town_Queenstown</th>\n",
       "      <th>embark_town_Southampton</th>\n",
       "      <th>alone</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.104637</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.478116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>1.893459</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.395814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-2.102733</td>\n",
       "      <td>2.247470</td>\n",
       "      <td>0.767630</td>\n",
       "      <td>-0.224083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.181487</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>2.008933</td>\n",
       "      <td>-0.424256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.369365</td>\n",
       "      <td>-1.180535</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.042956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  embarked_C  embarked_Q  embarked_S  class_First  class_Second  \\\n",
       "0  1.0         0.0         0.0         1.0          0.0           0.0   \n",
       "1  0.0         1.0         0.0         0.0          1.0           0.0   \n",
       "2  0.0         0.0         0.0         1.0          0.0           0.0   \n",
       "3  0.0         0.0         0.0         1.0          1.0           0.0   \n",
       "4  1.0         0.0         0.0         1.0          0.0           0.0   \n",
       "5  1.0         0.0         1.0         0.0          0.0           0.0   \n",
       "6  1.0         0.0         0.0         1.0          1.0           0.0   \n",
       "7  1.0         0.0         0.0         1.0          0.0           0.0   \n",
       "8  0.0         0.0         0.0         1.0          0.0           0.0   \n",
       "9  0.0         1.0         0.0         0.0          0.0           1.0   \n",
       "\n",
       "   class_Third  who_child  who_man  who_woman    ...     deck_G  \\\n",
       "0          1.0        0.0      1.0        0.0    ...        0.0   \n",
       "1          0.0        0.0      0.0        1.0    ...        0.0   \n",
       "2          1.0        0.0      0.0        1.0    ...        0.0   \n",
       "3          0.0        0.0      0.0        1.0    ...        0.0   \n",
       "4          1.0        0.0      1.0        0.0    ...        0.0   \n",
       "5          1.0        0.0      1.0        0.0    ...        0.0   \n",
       "6          0.0        0.0      1.0        0.0    ...        0.0   \n",
       "7          1.0        1.0      0.0        0.0    ...        0.0   \n",
       "8          1.0        0.0      0.0        1.0    ...        0.0   \n",
       "9          0.0        1.0      0.0        0.0    ...        0.0   \n",
       "\n",
       "   embark_town_Cherbourg  embark_town_Queenstown  embark_town_Southampton  \\\n",
       "0                    0.0                     0.0                      1.0   \n",
       "1                    1.0                     0.0                      0.0   \n",
       "2                    0.0                     0.0                      1.0   \n",
       "3                    0.0                     0.0                      1.0   \n",
       "4                    0.0                     0.0                      1.0   \n",
       "5                    0.0                     1.0                      0.0   \n",
       "6                    0.0                     0.0                      1.0   \n",
       "7                    0.0                     0.0                      1.0   \n",
       "8                    0.0                     0.0                      1.0   \n",
       "9                    1.0                     0.0                      0.0   \n",
       "\n",
       "   alone    pclass       age     sibsp     parch      fare  \n",
       "0    0.0  0.827377 -0.565736  0.432793 -0.473674 -0.502445  \n",
       "1    0.0 -1.566107  0.663861  0.432793 -0.473674  0.786845  \n",
       "2    1.0  0.827377 -0.258337 -0.474545 -0.473674 -0.488854  \n",
       "3    0.0 -1.566107  0.433312  0.432793 -0.473674  0.420730  \n",
       "4    1.0  0.827377  0.433312 -0.474545 -0.473674 -0.486337  \n",
       "5    1.0  0.827377 -0.104637 -0.474545 -0.473674 -0.478116  \n",
       "6    1.0 -1.566107  1.893459 -0.474545 -0.473674  0.395814  \n",
       "7    0.0  0.827377 -2.102733  2.247470  0.767630 -0.224083  \n",
       "8    0.0  0.827377 -0.181487 -0.474545  2.008933 -0.424256  \n",
       "9    0.0 -0.369365 -1.180535  0.432793 -0.473674 -0.042956  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper, CategoricalImputer, gen_features\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "X = df.copy().drop(['alive','survived'], axis = 'columns')\n",
    "y = df.survived\n",
    "\n",
    "# we need to set up transformations for numerical and categorical columns\n",
    "col_categorical = list( X.select_dtypes(exclude=np.number) )\n",
    "col_numerical   = list( X.select_dtypes(include=np.number) )\n",
    "\n",
    "#we need to convert to list of lists\n",
    "col_categorical = [ [x] for x in col_categorical ]\n",
    "col_numerical   = [ [x] for x in col_numerical ]\n",
    "\n",
    "# we have two ways of passing the classes as a simple list or as list of dicts if we need to pass\n",
    "# arguments as well\n",
    "classes_categorical = [ CategoricalImputer, sklearn.preprocessing.LabelBinarizer ]\n",
    "classes_numerical = [ {'class':sklearn.preprocessing.Imputer, 'strategy' : 'median'}\n",
    "                    , sklearn.preprocessing.StandardScaler\n",
    "                    ]\n",
    "\n",
    "# now that we have defined the columns and the classes of transformers we can use gen_features\n",
    "# in order to generate a list of tuples suitable for DataFrameMapper\n",
    "\n",
    "feature_def = gen_features(\n",
    "    columns = col_categorical\n",
    "    , classes = classes_categorical\n",
    ")\n",
    "\n",
    "feature_def_numerical = gen_features(\n",
    "    columns = col_numerical\n",
    "    , classes = classes_numerical\n",
    ")\n",
    "\n",
    "feature_def.extend(feature_def_numerical)\n",
    "\n",
    "# when constructing the mapper we can specify whether we want a dataframe or a numpy array as output\n",
    "\n",
    "mapper_df = DataFrameMapper( feature_def , df_out = True )\n",
    "\n",
    "mapper_np = DataFrameMapper( feature_def , df_out = False )\n",
    "\n",
    "mapped_df = mapper_df.fit_transform( df.copy() )\n",
    "\n",
    "mapped_np = mapper_np.fit_transform( df.copy() )\n",
    "\n",
    "print( mapped_np[1:10,1:20] )\n",
    "\n",
    "mapped_df.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results are looking really good, its almost as good as `recipes`. However if we wanted to apply a boxcox transformation on top of it we would have to write our own `scikit-learn` like transformer. However the transformer will be added in a future version so I would not bother with that at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `feather`\n",
    "The package `feather-format` uses `Apache arrow` to efficiently save dataframes on disk. It is compatible with many other programming languages including `R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "\n",
    "\n",
    "df_feather = mapped_df.\\\n",
    "    assign( y = y )\n",
    "\n",
    "feather.write_dataframe(df_feather, './data/mapped_df.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparcity ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparcity ratio original data: 0.17\n",
      "sparcity ratio tranformed data: 0.56\n"
     ]
    }
   ],
   "source": [
    "print('sparcity ratio original data:', round( sparsity_ratio(X), 2) )\n",
    "print('sparcity ratio tranformed data:', round( sparsity_ratio(mapped_np), 2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation have resulted in a matrix with a high sparcity thus we will test whether we might benefit from converting to a sparse matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec time sparse: 0.012\n",
      "exec time dense : 0.008\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from time import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "X_sparse = sparse.coo_matrix(mapped_np)\n",
    "\n",
    "clf_sparse = DecisionTreeClassifier()\n",
    "clf_dense = DecisionTreeClassifier()\n",
    "\n",
    "t0 = time()\n",
    "clf_sparse.fit(X_sparse, y)\n",
    "print('exec time sparse:', round( time() - t0,3 ) )\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "clf_dense.fit(mapped_np, y)\n",
    "print('exec time dense :', round( time() - t0,3 ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our decision tree classifiert does not benefit from  a sparse data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Pipelines are constructs that chain scikit preprocessing steps together and attaching an optional classifier or a regressor to the end. \n",
    "\n",
    "We can then use the pipe as we would use a regular model we can fit it and get predictions, we could get crossvalidated performance scores or perform parameter tuning. This has a couple of advantages.\n",
    "\n",
    "- The code becomes more compact and readable\n",
    "- Instead of saving multiple transformers (scaling, boxcox ) we can simply store one to apply to future data\n",
    "- We can tune several steps of the pipeline in one go (for example feature selector + model tuning parameters)\n",
    "\n",
    "We are going to contruct two pipes one for preprocessing and one for model fitting. It makes sense to seperate these two because we the first one contains a defined sequence of steps and the last pipe we are going to use to tune certain parameters via cross validation. \n",
    "\n",
    "When performning the cross validation the transformers and estimators in the pipe will be applied **after** splitting the data into cross validation pairs. Here it makes only sense to include steps that need to be unbiased such as feature selection and modelling algorithms. \n",
    "\n",
    "## Preprocessing Pipeline\n",
    "\n",
    "We are going to apply the `sklearn-pandas` dataframe mapper and a low variance feature filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('mapper', DataFrameMapper(default=False, df_out=False,\n",
       "        features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_la...h_std=True)])],\n",
       "        input_df=False, sparse=False)), ('feats', VarianceThreshold(threshold=0.0))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "\n",
    "pipe_pre_process = sklearn.pipeline.Pipeline([\n",
    "    ('mapper', mapper_np ) \n",
    "    , ('feats', VarianceThreshold() )\n",
    "])\n",
    "\n",
    "\n",
    "pipe_pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats': VarianceThreshold(threshold=0.0),\n",
       " 'mapper': DataFrameMapper(default=False, df_out=False,\n",
       "         features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['class'], [CategoricalImp...es='NaN', strategy='median', verbose=0), StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       "         input_df=False, sparse=False)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are saved as follows in a nested dictionary and are named after the following principle `step_name + '__' + argument`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats': VarianceThreshold(threshold=0.0),\n",
       " 'feats__threshold': 0.0,\n",
       " 'mapper': DataFrameMapper(default=False, df_out=False,\n",
       "         features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['class'], [CategoricalImp...es='NaN', strategy='median', verbose=0), StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       "         input_df=False, sparse=False),\n",
       " 'mapper__default': False,\n",
       " 'mapper__df_out': False,\n",
       " 'mapper__features': [(['sex'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['embarked'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['class'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['who'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['adult_male'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['deck'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['embark_town'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['alone'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['pclass'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['age'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['sibsp'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['parch'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['fare'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       " 'mapper__input_df': False,\n",
       " 'mapper__sparse': False,\n",
       " 'memory': None,\n",
       " 'steps': [('mapper', DataFrameMapper(default=False, df_out=False,\n",
       "           features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['class'], [CategoricalImp...es='NaN', strategy='median', verbose=0), StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       "           input_df=False, sparse=False)),\n",
       "  ('feats', VarianceThreshold(threshold=0.0))]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0.05)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.set_params(feats__threshold = 0.05)\n",
    "pipe_pre_process.named_steps.feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit the preprocessing pipe to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        , ...,  0.43279337,\n",
       "        -0.47367361, -0.50244517],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  0.43279337,\n",
       "        -0.47367361,  0.78684529],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.4745452 ,\n",
       "        -0.47367361, -0.48885426],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.43279337,\n",
       "         2.00893337, -0.17626324],\n",
       "       [ 1.        ,  1.        ,  0.        , ..., -0.4745452 ,\n",
       "        -0.47367361, -0.04438104],\n",
       "       [ 1.        ,  0.        ,  1.        , ..., -0.4745452 ,\n",
       "        -0.47367361, -0.49237783]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.fit(X)\n",
    "X_proc = pipe_pre_process.fit_transform(X)\n",
    "\n",
    "X_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Pipeline\n",
    "\n",
    "We will add a feature selection step, which choses variables based on a univariate test such as a chisquare test (which we cannot use here because it does not accept negative values) and ANOVA and then fit a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mod = sklearn.pipeline.Pipeline([\n",
    "    ('feats', sklearn.feature_selection.SelectKBest( k = 10) ) \n",
    "    , ('tree', sklearn.tree.DecisionTreeClassifier() )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same '__' synthax as we used for setting the parameters of the pipe for constructing the dictionary for the sandomized hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 500 candidates, totalling 15000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 165 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=4)]: Done 1408 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 3047 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done 4847 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=4)]: Done 7393 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=4)]: Done 10732 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=4)]: Done 14406 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=4)]: Done 15000 out of 15000 | elapsed: 16.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<sklearn.model_selection._split.RepeatedKFold object at 0x000001FFFEEABB00>,\n",
       "          error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('feats', SelectKBest(k=10, score_func=<function f_classif at 0x000001FFFCC336A8>)), ('tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))]),\n",
       "          fit_params=None, iid=True, n_iter=500, n_jobs=4,\n",
       "          param_distributions={'tree__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001FFFEEABBA8>, 'tree__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001FFFEEAB0F0>, 'tree__min_impurity_decrease': <scipy.stats._distn_infrastructure.rv_froze... [<function f_classif at 0x000001FFFCC336A8>, <function mutual_info_classif at 0x000001FFFCC59048>]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_dist = dict( tree__min_samples_split = stats.randint(2,250)\n",
    "                 , tree__min_samples_leaf = stats.randint(1,500)\n",
    "                 , tree__min_impurity_decrease = stats.uniform(0,1)\n",
    "                 , tree__max_features = stats.uniform(0,1)\n",
    "                 , feats__score_func = [sklearn.feature_selection.f_classif ## Anova\n",
    "                                       , sklearn.feature_selection.mutual_info_classif] ) ## nearest n\n",
    "\n",
    "n_iter = 500\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe_mod\n",
    "                                   , param_dist\n",
    "                                   , n_iter = n_iter\n",
    "                                   , scoring = 'roc_auc'\n",
    "                                   , cv = RepeatedKFold( n_splits = 5, n_repeats = 3 )\n",
    "                                   , verbose = True\n",
    "                                   , n_jobs = 4 ## parallel processing\n",
    "                                   , return_train_score = True\n",
    "                                  )\n",
    "\n",
    "\n",
    "random_search.fit(X = X_proc, y =  df.survived )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('feats', SelectKBest(k=10,\n",
       "      score_func=<function mutual_info_classif at 0x000001FFFCC59048>)), ('tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=0.700962458667292, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.003543609591600383,\n",
       "            min_impurity_split=None, min_samples_leaf=27,\n",
       "            min_samples_split=13, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
