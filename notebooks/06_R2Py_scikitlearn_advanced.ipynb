{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< image classes=\"center\" src=\"../../../r2py.png\" thumbnail=\"../../../r2py.png\" thumbnail-width=\"180px\" thumbnail-height=\"180px\">}}\n",
    "\n",
    "\n",
    "\n",
    "- [1 of 7: IDE]( {{< relref \"2018-08-21-r2py_ide.md\" >}}  )\n",
    "- [2 of 7: pandas]( {{< relref \"2018-08-22-r2py_pandas.md\" >}}  )\n",
    "- [3 of 7: matplotlib and seaborn]( {{< relref \"2018-08-23-r2py_matplotlib_seaborn.md\" >}}  )\n",
    "- [4 of 7: plotly]( {{< relref \"2018-08-24-r2py_plotly.md\" >}}  )\n",
    "- [5 of 7: scikitlearn]( {{< relref \"2018-08-25-r2py_scikitlearn.md\" >}}  )\n",
    "- [6 of 7: advanced scikitlearn]( {{< relref \"2018-08-26-r2py_scikitlearn_advanced.md\" >}}  )\n",
    "- [7 of 7: automated machine learning]( {{< relref \"2018-08-27-r2py_automated_ML.md\" >}}  )\n",
    "\n",
    "\n",
    "<!-- toc -->\n",
    "\n",
    "# Advanced scikitlearn\n",
    "\n",
    "In the last post, we have seen some advantages of `scikitlearn`. Most notably the seamless integration of parallel processing. I was struggeling a bit with the fact that `scikitlearn` only accepts `numpy` arrays as input and I was missing the `recipes` package which makes initial data transformation in `R` so much easier. Then I stumbled upon `sklearn-pandas` which seamlessly integrates `pandas` with `sklearn` without having to worry about numpy arrays and it supports a pipe based workflow, which is a `sklearn` feaure I have not started to explore yet. \n",
    "\n",
    "Apart from `sklearn-pandas` there are a number of projects that use the synthax and structure of scikit learn, a collection of them can be found at\n",
    "\n",
    "-[http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html](https://github.com/scikit-learn-contrib/scikit-learn-contrib/blob/master/README.md)\n",
    "\n",
    "-[http://scikit-learn.org/stable/related_projects.html](http://scikit-learn.org/stable/related_projects.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## `sklearn-pandas`\n",
    "\n",
    "- [github](https://github.com/scikit-learn-contrib/sklearn-pandas)\n",
    "\n",
    "Core of this package is the `DataFrameMapper` class which maps scikit learn Transformer classes to specific columns of a dataframe and outputs either a numpy array or dataframe.\n",
    "\n",
    "Additionally it provides a `CategoricalImputer` which accepts categorical data, which I had to write myself before in the last post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>class_First</th>\n",
       "      <th>class_Second</th>\n",
       "      <th>class_Third</th>\n",
       "      <th>who_child</th>\n",
       "      <th>who_man</th>\n",
       "      <th>who_woman</th>\n",
       "      <th>...</th>\n",
       "      <th>deck_G</th>\n",
       "      <th>embark_town_Cherbourg</th>\n",
       "      <th>embark_town_Queenstown</th>\n",
       "      <th>embark_town_Southampton</th>\n",
       "      <th>alone</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.104637</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.478116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>1.893459</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.395814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-2.102733</td>\n",
       "      <td>2.247470</td>\n",
       "      <td>0.767630</td>\n",
       "      <td>-0.224083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.181487</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>2.008933</td>\n",
       "      <td>-0.424256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.369365</td>\n",
       "      <td>-1.180535</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.042956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  embarked_C  embarked_Q  embarked_S  class_First  class_Second  \\\n",
       "0  1.0         0.0         0.0         1.0          0.0           0.0   \n",
       "1  0.0         1.0         0.0         0.0          1.0           0.0   \n",
       "2  0.0         0.0         0.0         1.0          0.0           0.0   \n",
       "3  0.0         0.0         0.0         1.0          1.0           0.0   \n",
       "4  1.0         0.0         0.0         1.0          0.0           0.0   \n",
       "5  1.0         0.0         1.0         0.0          0.0           0.0   \n",
       "6  1.0         0.0         0.0         1.0          1.0           0.0   \n",
       "7  1.0         0.0         0.0         1.0          0.0           0.0   \n",
       "8  0.0         0.0         0.0         1.0          0.0           0.0   \n",
       "9  0.0         1.0         0.0         0.0          0.0           1.0   \n",
       "\n",
       "   class_Third  who_child  who_man  who_woman    ...     deck_G  \\\n",
       "0          1.0        0.0      1.0        0.0    ...        0.0   \n",
       "1          0.0        0.0      0.0        1.0    ...        0.0   \n",
       "2          1.0        0.0      0.0        1.0    ...        0.0   \n",
       "3          0.0        0.0      0.0        1.0    ...        0.0   \n",
       "4          1.0        0.0      1.0        0.0    ...        0.0   \n",
       "5          1.0        0.0      1.0        0.0    ...        0.0   \n",
       "6          0.0        0.0      1.0        0.0    ...        0.0   \n",
       "7          1.0        1.0      0.0        0.0    ...        0.0   \n",
       "8          1.0        0.0      0.0        1.0    ...        0.0   \n",
       "9          0.0        1.0      0.0        0.0    ...        0.0   \n",
       "\n",
       "   embark_town_Cherbourg  embark_town_Queenstown  embark_town_Southampton  \\\n",
       "0                    0.0                     0.0                      1.0   \n",
       "1                    1.0                     0.0                      0.0   \n",
       "2                    0.0                     0.0                      1.0   \n",
       "3                    0.0                     0.0                      1.0   \n",
       "4                    0.0                     0.0                      1.0   \n",
       "5                    0.0                     1.0                      0.0   \n",
       "6                    0.0                     0.0                      1.0   \n",
       "7                    0.0                     0.0                      1.0   \n",
       "8                    0.0                     0.0                      1.0   \n",
       "9                    1.0                     0.0                      0.0   \n",
       "\n",
       "   alone    pclass       age     sibsp     parch      fare  \n",
       "0    0.0  0.827377 -0.565736  0.432793 -0.473674 -0.502445  \n",
       "1    0.0 -1.566107  0.663861  0.432793 -0.473674  0.786845  \n",
       "2    1.0  0.827377 -0.258337 -0.474545 -0.473674 -0.488854  \n",
       "3    0.0 -1.566107  0.433312  0.432793 -0.473674  0.420730  \n",
       "4    1.0  0.827377  0.433312 -0.474545 -0.473674 -0.486337  \n",
       "5    1.0  0.827377 -0.104637 -0.474545 -0.473674 -0.478116  \n",
       "6    1.0 -1.566107  1.893459 -0.474545 -0.473674  0.395814  \n",
       "7    0.0  0.827377 -2.102733  2.247470  0.767630 -0.224083  \n",
       "8    0.0  0.827377 -0.181487 -0.474545  2.008933 -0.424256  \n",
       "9    0.0 -0.369365 -1.180535  0.432793 -0.473674 -0.042956  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper, CategoricalImputer, gen_features\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "X = df.copy().drop(['alive','survived'], axis = 'columns')\n",
    "y = df.survived\n",
    "\n",
    "# we need to set up transformations for numerical and categorical columns\n",
    "col_categorical = list( X.select_dtypes(exclude=np.number) )\n",
    "col_numerical   = list( X.select_dtypes(include=np.number) )\n",
    "\n",
    "#we need to convert to list of lists\n",
    "col_categorical = [ [x] for x in col_categorical ]\n",
    "col_numerical   = [ [x] for x in col_numerical ]\n",
    "\n",
    "# we have two ways of passing the classes as a simple list or as list of dicts if we need to pass\n",
    "# arguments as well\n",
    "classes_categorical = [ CategoricalImputer, sklearn.preprocessing.LabelBinarizer ]\n",
    "classes_numerical = [ {'class':sklearn.preprocessing.Imputer, 'strategy' : 'median'}\n",
    "                    , sklearn.preprocessing.StandardScaler\n",
    "                    ]\n",
    "\n",
    "# now that we have defined the columns and the classes of transformers we can use gen_features\n",
    "# in order to generate a list of tuples suitable for DataFrameMapper\n",
    "\n",
    "feature_def = gen_features(\n",
    "    columns = col_categorical\n",
    "    , classes = classes_categorical\n",
    ")\n",
    "\n",
    "feature_def_numerical = gen_features(\n",
    "    columns = col_numerical\n",
    "    , classes = classes_numerical\n",
    ")\n",
    "\n",
    "feature_def.extend(feature_def_numerical)\n",
    "\n",
    "# when constructing the mapper we can specify whether we want a dataframe or a numpy array as output\n",
    "\n",
    "mapper_df = DataFrameMapper( feature_def , df_out = True )\n",
    "\n",
    "mapper_np = DataFrameMapper( feature_def , df_out = False )\n",
    "\n",
    "mapped_df = mapper_df.fit_transform( df.copy() )\n",
    "\n",
    "mapped_np = mapper_np.fit_transform( df.copy() )\n",
    "\n",
    "print( mapped_np[1:10,1:20] )\n",
    "\n",
    "mapped_df.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results are looking really good, its almost as good as `recipes`. However if we wanted to apply a boxcox transformation on top of it we would have to write our own `scikit-learn` like transformer. However the transformer will be added in a future version so I would not bother with that at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To sparse or not to sparse\n",
    "\n",
    "In the `python` data world data is considered to be *sparse* or *dense*. Which adresses the number of zeros in a matrix [wiki](https://en.wikipedia.org/wiki/Sparse_matrix). *sparse* means that you have a lot of them while *dense* means the opposite. There is no particular threshold but we should be aware that some data transformatios like dummy encoding make our data more *sparse*. A *sparse* matrix can be stored in a more memory efficient format such similar as a compressed image file and some algorithms can computationally leaverage this format to reduce computing time. [lasso](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_dense_vs_sparse_data.html) and [boosting gradient style algorithms](https://medium.com/sbc-group-blog/to-sparse-or-not-to-sparse-691483f87a53) seem to be able to profit from the sparse data format while others [neural nets, knn](https://medium.com/sbc-group-blog/to-sparse-or-not-to-sparse-691483f87a53) do not, and some like [randomForest](https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required) require the regular dense format and will otherwise raise an error. We can use `SciPy` to transform matrices to a dense format. We can measure the sparcity ratio as follows## Sparcity ratio\n",
    "\n",
    "### Sparcity Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_ratio(X):\n",
    "    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparcity ratio original data: 0.17\n",
      "sparcity ratio tranformed data: 0.56\n"
     ]
    }
   ],
   "source": [
    "print('sparcity ratio original data:', round( sparsity_ratio(X), 2) )\n",
    "print('sparcity ratio tranformed data:', round( sparsity_ratio(mapped_np), 2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation have resulted in a matrix with a high sparcity thus we will test whether we might benefit from converting to a sparse matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec time sparse: 0.019\n",
      "exec time dense : 0.008\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from time import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "X_sparse = sparse.coo_matrix(mapped_np)\n",
    "\n",
    "clf_sparse = DecisionTreeClassifier()\n",
    "clf_dense = DecisionTreeClassifier()\n",
    "\n",
    "t0 = time()\n",
    "clf_sparse.fit(X_sparse, y)\n",
    "print('exec time sparse:', round( time() - t0,3 ) )\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "clf_dense.fit(mapped_np, y)\n",
    "print('exec time dense :', round( time() - t0,3 ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our decision tree classifiert does not benefit from  a sparse data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipelines are constructs that chain scikit preprocessing steps together and attaching an optional classifier or a regressor to the end. \n",
    "\n",
    "We can then use the pipe as we would use a regular model we can fit it and get predictions, we could get crossvalidated performance scores or perform parameter tuning. This has a couple of advantages.\n",
    "\n",
    "- The code becomes more compact and readable\n",
    "- Instead of saving multiple transformers (scaling, boxcox ) we can simply store one to apply to future data\n",
    "- We can tune several steps of the pipeline in one go (for example feature selector + model tuning parameters)\n",
    "\n",
    "We are going to contruct two pipes one for preprocessing and one for model fitting. It makes sense to seperate these two because we the first one contains a defined sequence of steps and the last pipe we are going to use to tune certain parameters via cross validation. \n",
    "\n",
    "When performning the cross validation the transformers and estimators in the pipe will be applied **after** splitting the data into cross validation pairs. Cross validation is computationally expensive and we only want to use it for steps which are likely to introduce bias and can lead to overfitting such as feature selection and hyperparameter tuning.\n",
    "\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "We are going to apply the `sklearn-pandas` dataframe mapper and a low variance feature filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('mapper', DataFrameMapper(default=False, df_out=False,\n",
       "        features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_la...h_std=True)])],\n",
       "        input_df=False, sparse=False)), ('feats', VarianceThreshold(threshold=0.0))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "\n",
    "pipe_pre_process = sklearn.pipeline.Pipeline([\n",
    "    ('mapper', mapper_np ) \n",
    "    , ('feats', VarianceThreshold() )\n",
    "])\n",
    "\n",
    "\n",
    "pipe_pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats': VarianceThreshold(threshold=0.0),\n",
       " 'mapper': DataFrameMapper(default=False, df_out=False,\n",
       "         features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['class'], [CategoricalImp...es='NaN', strategy='median', verbose=0), StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       "         input_df=False, sparse=False)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are saved as follows in a nested dictionary and are named after the following principle `step_name + '__' + argument`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats': VarianceThreshold(threshold=0.0),\n",
       " 'feats__threshold': 0.0,\n",
       " 'mapper': DataFrameMapper(default=False, df_out=False,\n",
       "         features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['class'], [CategoricalImp...es='NaN', strategy='median', verbose=0), StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       "         input_df=False, sparse=False),\n",
       " 'mapper__default': False,\n",
       " 'mapper__df_out': False,\n",
       " 'mapper__features': [(['sex'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['embarked'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['class'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['who'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['adult_male'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['deck'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['embark_town'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['alone'],\n",
       "   [CategoricalImputer(copy=True, missing_values='NaN'),\n",
       "    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]),\n",
       "  (['pclass'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['age'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['sibsp'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['parch'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)]),\n",
       "  (['fare'],\n",
       "   [Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
       "    StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       " 'mapper__input_df': False,\n",
       " 'mapper__sparse': False,\n",
       " 'memory': None,\n",
       " 'steps': [('mapper', DataFrameMapper(default=False, df_out=False,\n",
       "           features=[(['sex'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['embarked'], [CategoricalImputer(copy=True, missing_values='NaN'), LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)]), (['class'], [CategoricalImp...es='NaN', strategy='median', verbose=0), StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       "           input_df=False, sparse=False)),\n",
       "  ('feats', VarianceThreshold(threshold=0.0))]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0.05)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.set_params(feats__threshold = 0.05)\n",
    "pipe_pre_process.named_steps.feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit the preprocessing pipe to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        , ...,  0.43279337,\n",
       "        -0.47367361, -0.50244517],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  0.43279337,\n",
       "        -0.47367361,  0.78684529],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.4745452 ,\n",
       "        -0.47367361, -0.48885426],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.43279337,\n",
       "         2.00893337, -0.17626324],\n",
       "       [ 1.        ,  1.        ,  0.        , ..., -0.4745452 ,\n",
       "        -0.47367361, -0.04438104],\n",
       "       [ 1.        ,  0.        ,  1.        , ..., -0.4745452 ,\n",
       "        -0.47367361, -0.49237783]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pre_process.fit(X)\n",
    "X_proc = pipe_pre_process.fit_transform(X)\n",
    "\n",
    "X_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the pre processed data in another post so we are saving it to disc. We are storing it in feather format which is basically hdfs which has much faster in terms of reading and writing from and to disc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "\n",
    "\n",
    "df_feather = mapped_df.\\\n",
    "    assign( y = y )\n",
    "\n",
    "feather.write_dataframe(df_feather, './data/mapped_df.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Pipeline\n",
    "\n",
    "We will add a feature selection step, which choses variables based on a univariate test such as a chisquare test (which we cannot use here because it does not accept negative values) and ANOVA and then fit a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mod = sklearn.pipeline.Pipeline([\n",
    "    ('feats', sklearn.feature_selection.SelectKBest( k = 10) ) \n",
    "    , ('tree', sklearn.tree.DecisionTreeClassifier() )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same '__' synthax as we used for setting the parameters of the pipe for constructing the dictionary for the sandomized hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 500 candidates, totalling 7500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done 740 tasks      | elapsed:   34.9s\n",
      "[Parallel(n_jobs=4)]: Done 2005 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 3737 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done 5556 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done 7241 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=4)]: Done 7493 out of 7500 | elapsed:  6.1min remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 7500 out of 7500 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<sklearn.model_selection._split.RepeatedKFold object at 0x000001916A6DC9B0>,\n",
       "          error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('feats', SelectKBest(k=10, score_func=<function f_classif at 0x000001916A6ABF28>)), ('tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))]),\n",
       "          fit_params=None, iid=True, n_iter=500, n_jobs=4,\n",
       "          param_distributions={'tree__min_impurity_decrease': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001916A6DC160>, 'feats__score_func': [<function f_classif at 0x000001916A6ABF28>, <function mutual_info_classif at 0x000001916A6CD2F0>], 'tree__max_features': <scipy.stats._distn_infrastru...tree__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001916A6D9FD0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_dist = dict( tree__min_samples_split = stats.randint(2,250)\n",
    "                 , tree__min_samples_leaf = stats.randint(1,500)\n",
    "                 , tree__min_impurity_decrease = stats.uniform(0,1)\n",
    "                 , tree__max_features = stats.uniform(0,1)\n",
    "                 , feats__score_func = [sklearn.feature_selection.f_classif ## Anova\n",
    "                                       , sklearn.feature_selection.mutual_info_classif] ) ## nearest n\n",
    "\n",
    "n_iter = 500\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe_mod\n",
    "                                   , param_dist\n",
    "                                   , n_iter = n_iter\n",
    "                                   , scoring = 'roc_auc'\n",
    "                                   , cv = RepeatedKFold( n_splits = 5, n_repeats = 3 )\n",
    "                                   , verbose = True\n",
    "                                   , n_jobs = 4 ## parallel processing\n",
    "                                   , return_train_score = True\n",
    "                                  )\n",
    "\n",
    "\n",
    "random_search.fit(X = X_proc, y =  df.survived )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('feats', SelectKBest(k=10,\n",
       "      score_func=<function mutual_info_classif at 0x000001916A6CD2F0>)), ('tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=0.6995334988533182, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.00253...t=47, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_feats__score_func</th>\n",
       "      <th>param_tree__max_features</th>\n",
       "      <th>param_tree__min_impurity_decrease</th>\n",
       "      <th>param_tree__min_samples_leaf</th>\n",
       "      <th>param_tree__min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.360021</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.851727</td>\n",
       "      <td>0.862858</td>\n",
       "      <td>&lt;function mutual_info_classif at 0x000001916A6...</td>\n",
       "      <td>0.699533</td>\n",
       "      <td>0.00253909</td>\n",
       "      <td>33</td>\n",
       "      <td>47</td>\n",
       "      <td>{'feats__score_func': &lt;function mutual_info_cl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772863</td>\n",
       "      <td>0.866083</td>\n",
       "      <td>0.847184</td>\n",
       "      <td>0.881330</td>\n",
       "      <td>0.867859</td>\n",
       "      <td>0.867060</td>\n",
       "      <td>0.031329</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.036028</td>\n",
       "      <td>0.016553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.004166</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.829953</td>\n",
       "      <td>0.838292</td>\n",
       "      <td>&lt;function f_classif at 0x000001916A6ABF28&gt;</td>\n",
       "      <td>0.87121</td>\n",
       "      <td>0.0133541</td>\n",
       "      <td>89</td>\n",
       "      <td>210</td>\n",
       "      <td>{'feats__score_func': &lt;function f_classif at 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736742</td>\n",
       "      <td>0.809940</td>\n",
       "      <td>0.845372</td>\n",
       "      <td>0.858085</td>\n",
       "      <td>0.798939</td>\n",
       "      <td>0.825771</td>\n",
       "      <td>0.006909</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.019111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.298661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.811322</td>\n",
       "      <td>0.811516</td>\n",
       "      <td>&lt;function mutual_info_classif at 0x000001916A6...</td>\n",
       "      <td>0.813777</td>\n",
       "      <td>0.0138638</td>\n",
       "      <td>124</td>\n",
       "      <td>79</td>\n",
       "      <td>{'feats__score_func': &lt;function mutual_info_cl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713745</td>\n",
       "      <td>0.779844</td>\n",
       "      <td>0.815275</td>\n",
       "      <td>0.821707</td>\n",
       "      <td>0.798939</td>\n",
       "      <td>0.825771</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032071</td>\n",
       "      <td>0.018513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.435979</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.777916</td>\n",
       "      <td>0.776460</td>\n",
       "      <td>&lt;function mutual_info_classif at 0x000001916A6...</td>\n",
       "      <td>0.407913</td>\n",
       "      <td>0.127602</td>\n",
       "      <td>3</td>\n",
       "      <td>215</td>\n",
       "      <td>{'feats__score_func': &lt;function mutual_info_cl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754735</td>\n",
       "      <td>0.786423</td>\n",
       "      <td>0.768997</td>\n",
       "      <td>0.766081</td>\n",
       "      <td>0.750393</td>\n",
       "      <td>0.787890</td>\n",
       "      <td>0.268832</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.019844</td>\n",
       "      <td>0.009008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.312650</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.777642</td>\n",
       "      <td>0.779836</td>\n",
       "      <td>&lt;function mutual_info_classif at 0x000001916A6...</td>\n",
       "      <td>0.95935</td>\n",
       "      <td>0.0960747</td>\n",
       "      <td>234</td>\n",
       "      <td>36</td>\n",
       "      <td>{'feats__score_func': &lt;function mutual_info_cl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713745</td>\n",
       "      <td>0.779844</td>\n",
       "      <td>0.779288</td>\n",
       "      <td>0.780731</td>\n",
       "      <td>0.750393</td>\n",
       "      <td>0.787890</td>\n",
       "      <td>0.020375</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.025972</td>\n",
       "      <td>0.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.777642</td>\n",
       "      <td>0.779836</td>\n",
       "      <td>&lt;function f_classif at 0x000001916A6ABF28&gt;</td>\n",
       "      <td>0.722103</td>\n",
       "      <td>0.0547579</td>\n",
       "      <td>104</td>\n",
       "      <td>224</td>\n",
       "      <td>{'feats__score_func': &lt;function f_classif at 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713745</td>\n",
       "      <td>0.779844</td>\n",
       "      <td>0.779288</td>\n",
       "      <td>0.780731</td>\n",
       "      <td>0.750393</td>\n",
       "      <td>0.787890</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.025972</td>\n",
       "      <td>0.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.777642</td>\n",
       "      <td>0.779836</td>\n",
       "      <td>&lt;function f_classif at 0x000001916A6ABF28&gt;</td>\n",
       "      <td>0.83097</td>\n",
       "      <td>0.0618748</td>\n",
       "      <td>211</td>\n",
       "      <td>14</td>\n",
       "      <td>{'feats__score_func': &lt;function f_classif at 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713745</td>\n",
       "      <td>0.779844</td>\n",
       "      <td>0.779288</td>\n",
       "      <td>0.780731</td>\n",
       "      <td>0.750393</td>\n",
       "      <td>0.787890</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.025972</td>\n",
       "      <td>0.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777642</td>\n",
       "      <td>0.779836</td>\n",
       "      <td>&lt;function f_classif at 0x000001916A6ABF28&gt;</td>\n",
       "      <td>0.93767</td>\n",
       "      <td>0.0945291</td>\n",
       "      <td>11</td>\n",
       "      <td>96</td>\n",
       "      <td>{'feats__score_func': &lt;function f_classif at 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713745</td>\n",
       "      <td>0.779844</td>\n",
       "      <td>0.779288</td>\n",
       "      <td>0.780731</td>\n",
       "      <td>0.750393</td>\n",
       "      <td>0.787890</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025972</td>\n",
       "      <td>0.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.773051</td>\n",
       "      <td>0.774232</td>\n",
       "      <td>&lt;function f_classif at 0x000001916A6ABF28&gt;</td>\n",
       "      <td>0.509988</td>\n",
       "      <td>0.0569047</td>\n",
       "      <td>74</td>\n",
       "      <td>12</td>\n",
       "      <td>{'feats__score_func': &lt;function f_classif at 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754735</td>\n",
       "      <td>0.786423</td>\n",
       "      <td>0.779288</td>\n",
       "      <td>0.780731</td>\n",
       "      <td>0.750393</td>\n",
       "      <td>0.787890</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.021037</td>\n",
       "      <td>0.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.004166</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.771509</td>\n",
       "      <td>0.774650</td>\n",
       "      <td>&lt;function f_classif at 0x000001916A6ABF28&gt;</td>\n",
       "      <td>0.555193</td>\n",
       "      <td>0.0792222</td>\n",
       "      <td>173</td>\n",
       "      <td>195</td>\n",
       "      <td>{'feats__score_func': &lt;function f_classif at 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713745</td>\n",
       "      <td>0.779844</td>\n",
       "      <td>0.779288</td>\n",
       "      <td>0.780731</td>\n",
       "      <td>0.750393</td>\n",
       "      <td>0.787890</td>\n",
       "      <td>0.006909</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>0.013231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "206       0.360021         0.001642         0.851727          0.862858   \n",
       "88        0.004166         0.001308         0.829953          0.838292   \n",
       "194       0.298661         0.000000         0.811322          0.811516   \n",
       "230       0.435979         0.003441         0.777916          0.776460   \n",
       "285       0.312650         0.001333         0.777642          0.779836   \n",
       "347       0.003999         0.001600         0.777642          0.779836   \n",
       "263       0.004570         0.001416         0.777642          0.779836   \n",
       "459       0.005208         0.000000         0.777642          0.779836   \n",
       "224       0.002137         0.001575         0.773051          0.774232   \n",
       "472       0.004166         0.002083         0.771509          0.774650   \n",
       "\n",
       "                               param_feats__score_func  \\\n",
       "206  <function mutual_info_classif at 0x000001916A6...   \n",
       "88          <function f_classif at 0x000001916A6ABF28>   \n",
       "194  <function mutual_info_classif at 0x000001916A6...   \n",
       "230  <function mutual_info_classif at 0x000001916A6...   \n",
       "285  <function mutual_info_classif at 0x000001916A6...   \n",
       "347         <function f_classif at 0x000001916A6ABF28>   \n",
       "263         <function f_classif at 0x000001916A6ABF28>   \n",
       "459         <function f_classif at 0x000001916A6ABF28>   \n",
       "224         <function f_classif at 0x000001916A6ABF28>   \n",
       "472         <function f_classif at 0x000001916A6ABF28>   \n",
       "\n",
       "    param_tree__max_features param_tree__min_impurity_decrease  \\\n",
       "206                 0.699533                        0.00253909   \n",
       "88                   0.87121                         0.0133541   \n",
       "194                 0.813777                         0.0138638   \n",
       "230                 0.407913                          0.127602   \n",
       "285                  0.95935                         0.0960747   \n",
       "347                 0.722103                         0.0547579   \n",
       "263                  0.83097                         0.0618748   \n",
       "459                  0.93767                         0.0945291   \n",
       "224                 0.509988                         0.0569047   \n",
       "472                 0.555193                         0.0792222   \n",
       "\n",
       "    param_tree__min_samples_leaf param_tree__min_samples_split  \\\n",
       "206                           33                            47   \n",
       "88                            89                           210   \n",
       "194                          124                            79   \n",
       "230                            3                           215   \n",
       "285                          234                            36   \n",
       "347                          104                           224   \n",
       "263                          211                            14   \n",
       "459                           11                            96   \n",
       "224                           74                            12   \n",
       "472                          173                           195   \n",
       "\n",
       "                                                params       ...         \\\n",
       "206  {'feats__score_func': <function mutual_info_cl...       ...          \n",
       "88   {'feats__score_func': <function f_classif at 0...       ...          \n",
       "194  {'feats__score_func': <function mutual_info_cl...       ...          \n",
       "230  {'feats__score_func': <function mutual_info_cl...       ...          \n",
       "285  {'feats__score_func': <function mutual_info_cl...       ...          \n",
       "347  {'feats__score_func': <function f_classif at 0...       ...          \n",
       "263  {'feats__score_func': <function f_classif at 0...       ...          \n",
       "459  {'feats__score_func': <function f_classif at 0...       ...          \n",
       "224  {'feats__score_func': <function f_classif at 0...       ...          \n",
       "472  {'feats__score_func': <function f_classif at 0...       ...          \n",
       "\n",
       "     split7_test_score  split7_train_score  split8_test_score  \\\n",
       "206           0.772863            0.866083           0.847184   \n",
       "88            0.736742            0.809940           0.845372   \n",
       "194           0.713745            0.779844           0.815275   \n",
       "230           0.754735            0.786423           0.768997   \n",
       "285           0.713745            0.779844           0.779288   \n",
       "347           0.713745            0.779844           0.779288   \n",
       "263           0.713745            0.779844           0.779288   \n",
       "459           0.713745            0.779844           0.779288   \n",
       "224           0.754735            0.786423           0.779288   \n",
       "472           0.713745            0.779844           0.779288   \n",
       "\n",
       "     split8_train_score  split9_test_score  split9_train_score  std_fit_time  \\\n",
       "206            0.881330           0.867859            0.867060      0.031329   \n",
       "88             0.858085           0.798939            0.825771      0.006909   \n",
       "194            0.821707           0.798939            0.825771      0.029102   \n",
       "230            0.766081           0.750393            0.787890      0.268832   \n",
       "285            0.780731           0.750393            0.787890      0.020375   \n",
       "347            0.780731           0.750393            0.787890      0.002529   \n",
       "263            0.780731           0.750393            0.787890      0.006060   \n",
       "459            0.780731           0.750393            0.787890      0.009316   \n",
       "224            0.780731           0.750393            0.787890      0.001999   \n",
       "472            0.780731           0.750393            0.787890      0.006909   \n",
       "\n",
       "     std_score_time  std_test_score  std_train_score  \n",
       "206        0.003972        0.036028         0.016553  \n",
       "88         0.003953        0.034927         0.019111  \n",
       "194        0.000000        0.032071         0.018513  \n",
       "230        0.005945        0.019844         0.009008  \n",
       "285        0.001885        0.025972         0.005002  \n",
       "347        0.001959        0.025972         0.005002  \n",
       "263        0.002616        0.025972         0.005002  \n",
       "459        0.000000        0.025972         0.005002  \n",
       "224        0.003991        0.021037         0.015873  \n",
       "472        0.005311        0.027898         0.013231  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame( random_search.cv_results_ )\n",
    "\n",
    "res.sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We can see that our maximum ROC score 0.86 is similar to what we obtained in the last post (0.85) where we took a more manual approach. However using `sklearn-pandas` and pipes we were able to write code that is more generalizable and is less dependent on the actual dataset. We have more or less written a generalizable code for the preprocessing pipe however the code for the modelling pipe is quite specific for the model that we used. I f we wanted to train more models of a different type we would have to manually write pipes for them as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
